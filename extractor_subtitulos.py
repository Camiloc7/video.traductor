"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         EXTRACTOR DE SUBTÃTULOS CHINO â†’ ESPAÃ‘OL  (v2.0 - CPU Optimizado)   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  InstalaciÃ³n de dependencias:                                                â•‘
â•‘                                                                              â•‘
â•‘  pip install paddlepaddle paddleocr                                          â•‘
â•‘  pip install transformers sentencepiece sacremoses                           â•‘
â•‘  pip install opencv-python tqdm torch                                        â•‘
â•‘                                                                              â•‘
â•‘  Nota: En la PRIMERA ejecuciÃ³n se descargarÃ¡n automÃ¡ticamente:               â•‘
â•‘    - Modelos PaddleOCR para chino (~150MB)                                   â•‘
â•‘    - Modelo NLLB-200 distilled 600M (~2.4GB)                                 â•‘
â•‘  Las siguientes ejecuciones usan cachÃ© local, sin re-descarga.               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import re
# IMPORTANTE: Torch debe importarse ANTES que PaddleOCR en Windows para evitar conflictos de DLLs (WinError 127)
import torch
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import cv2
import time
import json
import hashlib
import threading
import logging
import warnings
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import deque
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  CONFIGURACIÃ“N DEL PROYECTO  (edita aquÃ­ tus parÃ¡metros)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VIDEO_PATH      = 'video_chino.mp4'
SRT_OUTPUT      = 'traduccion_final.srt'

# RegiÃ³n de interÃ©s (ROI) â€” zona donde aparecen los subtÃ­tulos en el video
Y_INICIO = 840
Y_FIN    = 960
X_INICIO = 10
X_FIN    = 710   # Video es 720 wide

# Salto de frames: analiza 1 frame cada N milisegundos (50ms = captura de subtÃ­tulos muy rÃ¡pidos)
FRAME_SKIP_MS   = 47

# Densidad mÃ­nima de pÃ­xeles de texto para considerar que hay subtÃ­tulo
DENSIDAD_MIN    = 0.001

# MÃ¡xima pausa entre subtÃ­tulos iguales para fusionarlos (en ms)
FUSION_GAP_MS   = 1500

# Archivos de estado y cachÃ©
PROGRESS_FILE   = 'progreso_traduccion.json'
OCR_CACHE_FILE  = 'ocr_cache.json'
TRANS_CACHE_FILE= 'translation_cache.json'

# CuÃ¡ntas futures acumular antes de recolectarlas (evita consumo excesivo de RAM)
FUTURES_BUFFER  = 100

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  SILENCIAR LOGS INNECESARIOS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("ppocr").setLevel(logging.ERROR)
logging.getLogger("paddle").setLevel(logging.ERROR)
warnings.filterwarnings("ignore")
os.environ["GLOG_minloglevel"] = "3"   # Silencia logs de PaddlePaddle en C++
os.environ["PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK"] = "True" 
os.environ["FLAGS_enable_pir_api"] = "0"    # Desactiva el nuevo motor PIR (mÃ¡s estable en CPU AMD)
os.environ["FLAGS_use_mkldnn"] = "0"        # Desactiva MKLDNN a nivel de sistema

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  CACHÃ‰S EN MEMORIA + DISCO
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
translation_cache: dict = {}
ocr_cache: dict         = {}
ocr_lock   = threading.Lock()
trans_lock = threading.Lock()

def cargar_caches():
    global ocr_cache, translation_cache
    for path, ref in [(OCR_CACHE_FILE, 'ocr'), (TRANS_CACHE_FILE, 'trans')]:
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if ref == 'ocr':
                    ocr_cache = data
                else:
                    translation_cache = data
                print(f"  âœ” CachÃ© cargada: {path} ({len(data)} entradas)")
            except Exception as e:
                print(f"  âš  No se pudo leer {path}: {e}")

def normalizar_texto_cache(texto: str) -> str:
    """Genera una clave Ãºnica y limpia para el cachÃ© de traducciÃ³n."""
    if not texto: return ""
    texto = texto.strip()
    # quitar puntuaciÃ³n final comÃºn (china y latina)
    texto = re.sub(r'[ã€‚.!ï¼?ï¼Ÿ,ï¼Œ]+$', '', texto)
    # quitar ABSOLUTAMENTE todo espacio en blanco para que "A B" y "AB" sean la misma clave
    texto = re.sub(r'\s+', '', texto)
    return texto.lower()

def guardar_caches():
    with ocr_lock:
        with open(OCR_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(ocr_cache, f, ensure_ascii=False, indent=2)
    with trans_lock:
        with open(TRANS_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(translation_cache, f, ensure_ascii=False, indent=2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  INICIALIZACIÃ“N DE MODELOS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cargar_caches()

# â”€â”€ 1. PaddleOCR (reemplaza EasyOCR â€” mucho mÃ¡s rÃ¡pido en CPU) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[1/2] Inicializando PaddleOCR (CPU, chino simplificado)...")
from paddleocr import PaddleOCR

ocr_engine = PaddleOCR(
    use_angle_cls=False,   # Los subtÃ­tulos siempre estÃ¡n horizontales
    lang='ch',
    use_gpu=False,
    show_log=False,
)
print("  âœ” PaddleOCR listo.")

# â”€â”€ 2. NLLB-200 Distilled (traducciÃ³n con idioma pivote: ZH â†’ EN â†’ ES) â”€â”€â”€â”€â”€
print("\n[2/2] Cargando NLLB-200 distilled-600M (primera vez descarga ~2.4GB)...")
# Los modelos ya fueron importados arriba

NLLB_MODEL = "facebook/nllb-200-distilled-600M"
nllb_tokenizer = AutoTokenizer.from_pretrained(NLLB_MODEL)
nllb_model     = AutoModelForSeq2SeqLM.from_pretrained(NLLB_MODEL)

# â”€â”€ QuantizaciÃ³n dinÃ¡mica: reduce RAM ~40% y acelera inferencia en CPU â”€â”€â”€â”€â”€â”€â”€â”€
nllb_model = torch.quantization.quantize_dynamic(
    nllb_model,
    {torch.nn.Linear},
    dtype=torch.qint8
)
nllb_model.eval()
nllb_model.config.use_cache = True
print("  âœ” NLLB-200 listo (con quantizaciÃ³n int8 y cache activado).")

# Candado para traducciÃ³n (el modelo no es thread-safe sin esto)
nllb_lock = threading.Lock()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  FUNCIONES AUXILIARES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def limpiar_texto(t: str) -> str:
    """Elimina ruidos comunes del OCR y caracteres que ensucian la traducciÃ³n."""
    if not t: return ""
    # Eliminar sÃ­mbolos raros, guiones bajos extraÃ±os, etc.
    t = re.sub(r'[_|\\[\\]{}#]', '', t)
    # Eliminar espacios mÃºltiples
    t = re.sub(r'\s+', ' ', t).strip()
    return t

def traducir_batch(textos: list, batch_size: int = 8) -> list:
    """
    Traduce una lista de textos usando batching de NLLB para mayor velocidad.
    """
    if not textos:
        return []

    resultados = [""] * len(textos)
    pendientes_idx = []
    pendientes_txt = []

    # 1. Limpieza y revisiÃ³n de cachÃ©
    for i, t in enumerate(textos):
        t_limpio = limpiar_texto(t)
        if not t_limpio:
            continue
        
        t_key = normalizar_texto_cache(t_limpio)
        with trans_lock:
            cached = translation_cache.get(t_key)
        
        if cached:
            resultados[i] = cached
        else:
            pendientes_idx.append(i)
            # Guardamos el texto "limpio" (con espacios) para traducir, 
            # pero usaremos la t_key para guardarlo despuÃ©s
            pendientes_txt.append(t_limpio)

    if not pendientes_txt:
        return resultados

    # --- TRUCO 1: Ordenar por longitud para reducir padding ---
    indices_ordenados = sorted(
        range(len(pendientes_txt)),
        key=lambda i: len(pendientes_txt[i])
    )
    pendientes_txt = [pendientes_txt[i] for i in indices_ordenados]
    pendientes_idx = [pendientes_idx[i] for i in indices_ordenados]

    # 2. TraducciÃ³n en lotes
    pbar = tqdm(total=len(pendientes_txt), desc="Traduciendo (NLLB Batch)", unit="txt", leave=False)
    for i in range(0, len(pendientes_txt), batch_size):
        lote = pendientes_txt[i : i + batch_size]
        lote_idx = pendientes_idx[i : i + batch_size]

        try:
            with nllb_lock:
                with torch.inference_mode():
                    inputs = nllb_tokenizer(
                        lote,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=256,
                        src_lang="zho_Hans"
                    )

                    # --- TRUCO 4: max_length inteligente ---
                    out_len = min(256, int(inputs.input_ids.shape[1] * 2.5))

                    # --- TRUCO 2: Greedy search (num_beams=1) ---
                    tokens = nllb_model.generate(
                        **inputs,
                        forced_bos_token_id=nllb_tokenizer.lang_code_to_id["spa_Latn"],
                        max_length=out_len,
                        do_sample=False,
                        num_beams=1,
                        use_cache=True,
                        repetition_penalty=1.1
                    )

            traducciones = nllb_tokenizer.batch_decode(tokens, skip_special_tokens=True)

                for j, traducido in enumerate(traducciones):
                traducido = traducido.strip()
                real_idx = lote_idx[j]
                resultados[real_idx] = traducido
                
                # Guardar en cachÃ© usando la clave normalizada
                t_key = normalizar_texto_cache(pendientes_txt[i + j])
                with trans_lock:
                    translation_cache[t_key] = traducido

            pbar.update(len(lote))

        except Exception as e:
            print(f"  âš  Error en batch translation: {e}")
    
    pbar.close()

    return resultados

def traducir_zh_es(texto: str) -> str:
    """Wrapper para mantener compatibilidad con llamadas individuales."""
    res = traducir_batch([texto], batch_size=1)
    return res[0] if res else ""

def preprocesar_roi(roi_bgr: "np.ndarray") -> "np.ndarray":
    """
    Pipeline de preprocesamiento mejorado para PaddleOCR.
    """
    import cv2
    import numpy as np

    # 1. Escalar primero (mejor para preservar bordes en OCR)
    h, w = roi_bgr.shape[:2]
    roi_large = cv2.resize(roi_bgr, (int(w * 2), int(h * 2)), interpolation=cv2.INTER_CUBIC)

    # 2. ReducciÃ³n de ruido preservando bordes
    denoised = cv2.bilateralFilter(roi_large, 9, 75, 75)

    # 3. Grayscale
    gray = cv2.cvtColor(denoised, cv2.COLOR_BGR2GRAY)

    # 4. Sharpening (MÃ¡scara de enfoque)
    # Aumenta el contraste de los bordes del texto
    blurred = cv2.GaussianBlur(gray, (0, 0), 3)
    sharpened = cv2.addWeighted(gray, 1.5, blurred, -0.5, 0)

    return sharpened


def densidad_texto(roi_bgr: "np.ndarray") -> float:
    """Calcula la densidad de pÃ­xeles de texto en la ROI (0.0 - 1.0)."""
    import cv2
    gray = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2GRAY)
    mask  = cv2.inRange(gray, 180, 255)
    edges = cv2.Canny(gray, 30, 100)
    posible_texto = cv2.bitwise_and(edges, mask)
    return cv2.countNonZero(posible_texto) / posible_texto.size


def ms_to_srt_time(ms: float) -> str:
    ms = int(ms)
    h  = ms // 3_600_000; ms %= 3_600_000
    m  = ms //    60_000; ms %=    60_000
    s  = ms //     1_000; ms %=     1_000
    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"


def cargar_progreso():
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
                d = json.load(f)
            return d.get("last_frame", 0), d.get("subtitles", [])
        except Exception:
            pass
    return 0, []


def guardar_progreso(last_frame: int, subs: list):
    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
        json.dump({"last_frame": last_frame, "subtitles": subs}, f,
                  ensure_ascii=False, indent=2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  WORKER: OCR + TRADUCCIÃ“N (ejecutado en hilo secundario)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def procesar_subtitulo(roi_preprocesada, inicio_ms: float, fin_ms: float, img_hash: str):
    """
    Worker que corre en el ThreadPoolExecutor.
    1. OCR con PaddleOCR (con cachÃ© por hash de imagen)
    2. TraducciÃ³n con NLLB-200 (con cachÃ© por texto)
    """
    # â”€â”€ OCR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    texto_ori = None
    with ocr_lock:
        texto_ori = ocr_cache.get(img_hash)

    if texto_ori is None:
        try:
            resultado = ocr_engine.ocr(roi_preprocesada)
            lineas = []
            if resultado and resultado[0]:
                for linea in resultado[0]:
                    # linea[1] = (texto, confianza)
                    if linea[1][1] >= 0.5:          # Descartar detecciones de baja confianza
                        lineas.append(linea[1][0])
            texto_ori = " ".join(lineas).strip()
        except Exception as e:
            print(f"\n  âš  Error OCR: {e}")
            texto_ori = ""

        if texto_ori:
            with ocr_lock:
                ocr_cache[img_hash] = texto_ori

    if not texto_ori or len(texto_ori) < 2:
        return None

    # â”€â”€ TraducciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # La traducciÃ³n individual se eliminÃ³ para usar la traducciÃ³n por bloques al final
    return {
        'inicio':     inicio_ms,
        'fin':        fin_ms,
        'texto_ori':  texto_ori,
        'texto_final': "",
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  FUNCIÃ“N PRINCIPAL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def traducir_por_bloques(
    subs,
    max_gap_ms=4000,
    max_chars=300,
    max_items=16
):
    """
    Traduce subtÃ­tulos agrupÃ¡ndolos en bloques grandes con contexto.

    max_gap_ms  = mÃ¡ximo espacio temporal entre subtÃ­tulos del mismo bloque
    max_chars   = mÃ¡ximo caracteres concatenados por bloque
    max_items   = mÃ¡ximo subtÃ­tulos por bloque
    """

    if not subs:
        return []

    bloques = []
    bloque_actual = []
    chars_actual = 0

    for sub in subs:

        texto = sub.get("texto_ori", "").strip()

        if not texto:
            continue

        if not bloque_actual:
            bloque_actual.append(sub)
            chars_actual = len(texto)
            continue

        ultimo = bloque_actual[-1]

        gap = sub["inicio"] - ultimo["fin"]

        # decidir si agregar al bloque actual
        if (
            gap <= max_gap_ms
            and chars_actual + len(texto) <= max_chars
            and len(bloque_actual) < max_items
        ):
            bloque_actual.append(sub)
            chars_actual += len(texto)

        else:
            bloques.append(bloque_actual)
            bloque_actual = [sub]
            chars_actual = len(texto)

    if bloque_actual:
        bloques.append(bloque_actual)

    resultado_final = []

    # --- NUEVA LÃ“GICA DE BATCHING ---
    textos_concat = []
    for bloque in bloques:
        txt = " ".join(limpiar_texto(b["texto_ori"]) for b in bloque).strip()
        textos_concat.append(txt)

    print(f"\nğŸ“¦ Traduciendo {len(bloques)} bloques en modo Batch (Turbo)...")
    # Aumentamos a 8 para aprovechar los trucos de optimizaciÃ³n
    traducciones_lote = traducir_batch(textos_concat, batch_size=8)

    for bloque, traduccion in zip(bloques, traducciones_lote):
        if not traduccion:
            resultado_final.extend(bloque)
            continue

        # Dividir la traducciÃ³n resultante entre los subtÃ­tulos del bloque
        palabras = traduccion.split()
        total_palabras = len(palabras)
        total_subs = len(bloque)

        if total_subs == 1:
            bloque[0]["texto_final"] = traduccion
            resultado_final.append(bloque[0])
            continue

        # DistribuciÃ³n proporcional de palabras
        ratio = total_palabras / total_subs
        idx = 0
        for i, sub in enumerate(bloque):
            if i == total_subs - 1:
                segmento = palabras[idx:]
            else:
                n = max(1, round(ratio))
                segmento = palabras[idx:idx+n]
                idx += n
            sub["texto_final"] = " ".join(segmento)
            resultado_final.append(sub)

    return resultado_final




def procesar_video():
    import cv2
    import numpy as np

    cap = cv2.VideoCapture(VIDEO_PATH)
    if not cap.isOpened():
        print(f"\nâŒ Error: No se pudo abrir el video en '{VIDEO_PATH}'")
        return

    fps          = cap.get(cv2.CAP_PROP_FPS) or 25.0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duracion_seg = total_frames / fps

    # Convertir salto de ms a nÃºmero de frames
    FRAME_SKIP = max(1, int(fps * FRAME_SKIP_MS / 1000))

    print(f"\nğŸ“¹ Video: {VIDEO_PATH}")
    print(f"   FPS: {fps:.2f} | Frames totales: {total_frames} | DuraciÃ³n: {duracion_seg/60:.1f} min")
    print(f"   Analizando 1 frame cada {FRAME_SKIP_MS}ms ({FRAME_SKIP} frames)")
    print(f"   ROI: Y[{Y_INICIO}:{Y_FIN}] X[{X_INICIO}:{X_FIN}]")

    # â”€â”€ Cargar checkpoint si existe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    last_frame, subtitulos_completados = cargar_progreso()
    if last_frame > 0:
        print(f"\nğŸ”„ Reanudando desde el frame {last_frame} ({last_frame/fps/60:.1f} min)...")
        cap.set(cv2.CAP_PROP_POS_FRAMES, last_frame)

    # â”€â”€ Estado de la mÃ¡quina de estados â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    en_subtitulo   = False
    inicio_sub     = 0.0
    mejor_roi      = None
    max_densidad   = 0.0

    futures        = []
    WORKERS        = max(2, (os.cpu_count() or 4) - 1)
    print(f"\nâš¡ Usando {WORKERS} hilos de trabajo\n")

    with ThreadPoolExecutor(max_workers=WORKERS) as executor:
        pbar = tqdm(total=total_frames, initial=last_frame,
                    desc="Procesando", unit="fr",
                    bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]")
        try:
            while True:
                pos_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
                pbar.update(pos_frame - pbar.n)

                # grab() es rÃ¡pido (no decodifica) â€” solo decodifica frames que necesitamos
                ret = cap.grab()
                if not ret:
                    break

                if pos_frame % FRAME_SKIP != 0:
                    continue

                ret, frame = cap.retrieve()
                if not ret:
                    continue

                tiempo_ms = cap.get(cv2.CAP_PROP_POS_MSEC)

                try:
                    roi = frame[Y_INICIO:Y_FIN, X_INICIO:X_FIN]
                    densidad = densidad_texto(roi)

                    if densidad >= DENSIDAD_MIN:
                        # â”€â”€ Hay texto en pantalla â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                        roi_proc = preprocesar_roi(roi)

                        if not en_subtitulo:
                            en_subtitulo = True
                            inicio_sub   = tiempo_ms
                            mejor_roi    = roi_proc.copy()
                            max_densidad = densidad
                        else:
                            # Guardar el frame con mÃ¡s densidad de texto
                            if densidad > max_densidad:
                                max_densidad = densidad
                                mejor_roi    = roi_proc.copy()

                    else:
                        # â”€â”€ Fin de subtÃ­tulo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                        if en_subtitulo:
                            en_subtitulo = False
                            fin_sub      = tiempo_ms

                            if mejor_roi is not None:
                                img_hash = hashlib.md5(mejor_roi.tobytes()).hexdigest()

                                # Verificar cachÃ© antes de lanzar hilo
                                texto_conocido = None
                                with ocr_lock:
                                    texto_conocido = ocr_cache.get(img_hash)

                                if texto_conocido and len(texto_conocido) >= 2:
                                    subtitulos_completados.append({
                                        'inicio':     inicio_sub,
                                        'fin':        fin_sub,
                                        'texto_ori':  texto_conocido,
                                        'texto_final': "",
                                    })
                                else:
                                    future = executor.submit(
                                        procesar_subtitulo,
                                        mejor_roi.copy(), inicio_sub, fin_sub, img_hash
                                    )
                                    futures.append(future)

                            mejor_roi    = None
                            max_densidad = 0.0

                except Exception:
                    continue

                # â”€â”€ RecolecciÃ³n periÃ³dica de futures completadas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                if len(futures) >= FUTURES_BUFFER:
                    completadas = [f for f in futures if f.done()]
                    for f in completadas:
                        try:
                            res = f.result()
                            if res:
                                subtitulos_completados.append(res)
                                tqdm.write(f"  ğŸ’¬ {res['texto_ori'][:40]} â†’ {res['texto_final'][:60]}")
                        except Exception as e:
                            tqdm.write(f"  âš  Error en worker: {e}")
                        futures.remove(f)

                    if completadas:
                        guardar_progreso(pos_frame, subtitulos_completados)
                        guardar_caches()

        except KeyboardInterrupt:
            tqdm.write("\nâ¸ Proceso pausado por el usuario. Guardando progreso...")

        finally:
            pbar.close()

            # Si el video terminÃ³ con un subtÃ­tulo activo, procesarlo
            if en_subtitulo and mejor_roi is not None:
                img_hash = hashlib.md5(mejor_roi.tobytes()).hexdigest()
                futures.append(executor.submit(
                    procesar_subtitulo,
                    mejor_roi, inicio_sub, cap.get(cv2.CAP_PROP_POS_MSEC), img_hash
                ))

            # Esperar y recolectar TODAS las futures pendientes
            if futures:
                print(f"\nâ³ Finalizando {len(futures)} tareas pendientes...")
                pbar_sync = tqdm(total=len(futures), desc="Finalizando OCR/TraducciÃ³n", unit="sub")
                for f in as_completed(futures):
                    try:
                        res = f.result()
                        if res:
                            subtitulos_completados.append(res)
                    except Exception as e:
                        print(f"  âš  Error en worker final: {e}")
                    pbar_sync.update(1)
                pbar_sync.close()

            pos_final = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
            cap.release()
            cv2.destroyAllWindows()

    # â”€â”€ Post-proceso: ordenar + fusionar subtÃ­tulos duplicados â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subtitulos_completados.sort(key=lambda x: x['inicio'])

    subtitulos_fusionados = []
    for sub in subtitulos_completados:
        if not subtitulos_fusionados:
            subtitulos_fusionados.append(dict(sub))
            continue
        ultimo = subtitulos_fusionados[-1]
        # Fusionar si mismo texto y brecha pequeÃ±a
        mismo_texto = sub['texto_ori'] == ultimo['texto_ori']
        brecha_ok   = (sub['inicio'] - ultimo['fin']) < FUSION_GAP_MS
        if mismo_texto and brecha_ok:
            ultimo['fin'] = sub['fin']   # Extender duraciÃ³n
        else:
            subtitulos_fusionados.append(dict(sub))

    print("\n[2.5/3] Traduciendo por bloques con contexto...")
    subtitulos_fusionados = traducir_por_bloques(subtitulos_fusionados)

    # â”€â”€ Post-proceso 2: Reparar traducciones vacÃ­as por contexto â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n[3/3] Reparando traducciones en blanco mediante contexto...")
    reparados = []
    i = 0
    pbar_rep = tqdm(total=len(subtitulos_fusionados), desc="Reparando", unit="sub")
    
    while i < len(subtitulos_fusionados):
        sub = subtitulos_fusionados[i]
        txt_trad = sub.get('texto_final') or ""
        txt_trad = txt_trad.strip()
        
        if not txt_trad:
            fusionado = False
            # 1. Intentar con el ANTERIOR
            if reparados:
                ultimo = reparados[-1]
                if (sub['inicio'] - ultimo['fin']) < (FUSION_GAP_MS * 1.5):
                    nuevo_ori = (ultimo.get('texto_ori', '') + " " + sub.get('texto_ori', '')).strip()
                    n_key = normalizar_texto_cache(nuevo_ori)
                    
                    with trans_lock:
                        nueva_trad = translation_cache.get(n_key)
                    
                    if not nueva_trad:
                        nueva_trad = traducir_zh_es(nuevo_ori)
                        # traducir_zh_es ya guarda en cachÃ© internamente
                    
                    if nueva_trad:
                        ultimo['texto_ori'] = nuevo_ori
                        ultimo['texto_final'] = nueva_trad
                        ultimo['fin'] = max(ultimo['fin'], sub['fin'])
                        fusionado = True
                        tqdm.write(f"  ğŸ”§ Reparado (<- anterior): {nueva_trad}")
            
            # 2. Intentar con el SIGUIENTE
            if not fusionado and i + 1 < len(subtitulos_fusionados):
                siguiente = subtitulos_fusionados[i+1]
                if (siguiente['inicio'] - sub['fin']) < (FUSION_GAP_MS * 1.5):
                    nuevo_ori = (sub.get('texto_ori', '') + " " + siguiente.get('texto_ori', '')).strip()
                    n_key = normalizar_texto_cache(nuevo_ori)
                    
                    with trans_lock:
                        nueva_trad = translation_cache.get(n_key)
                        
                    if not nueva_trad:
                        nueva_trad = traducir_zh_es(nuevo_ori)
                            
                    if nueva_trad:
                        siguiente['texto_ori'] = nuevo_ori
                        siguiente['texto_final'] = nueva_trad
                        siguiente['inicio'] = min(siguiente['inicio'], sub['inicio'])
                        fusionado = True
                        tqdm.write(f"  ğŸ”§ Reparado (-> siguiente): {nueva_trad}")
            
            if not fusionado:
                reparados.append(sub)
        else:
            reparados.append(sub)
            
        i += 1
        pbar_rep.update(1)
        
    pbar_rep.close()
    subtitulos_fusionados = reparados

    # Asignar IDs secuenciales finales
    for idx, sub in enumerate(subtitulos_fusionados, start=1):
        sub['id'] = idx

    # â”€â”€ Guardar estado y cachÃ©s finales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    guardar_progreso(pos_final, subtitulos_fusionados)
    guardar_caches()

    # â”€â”€ Escribir archivo SRT (solo subtÃ­tulos con texto vÃ¡lido) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with open(SRT_OUTPUT, 'w', encoding='utf-8') as f:
        real_idx = 1
        for sub in subtitulos_fusionados:
            txt = sub.get('texto_final', '').strip()
            # Si no hay traducciÃ³n, intentar usar el original si no es basura
            if not txt:
                cont_zh = sub.get('texto_ori', '')
                if len(cont_zh) > 1 and not re.search(r'^[^\w\u4e00-\u9fff]+$', cont_zh):
                    txt = f"[{cont_zh}]" # Entre corchetes si no se pudo traducir
            
            if txt:
                f.write(f"{real_idx}\n")
                f.write(f"{ms_to_srt_time(sub['inicio'])} --> {ms_to_srt_time(sub['fin'])}\n")
                f.write(f"{txt}\n\n")
                real_idx += 1

    print(f"\n{'â•'*60}")
    print(f"âœ… Â¡Proceso completado!")
    print(f"   SubtÃ­tulos encontrados: {len(subtitulos_fusionados)}")
    print(f"   Archivo generado: {SRT_OUTPUT}")
    print(f"{'â•'*60}\n")

    # Limpiar checkpoint si se procesÃ³ todo el video
    if pos_final >= total_frames - FRAME_SKIP * 2:
        if os.path.exists(PROGRESS_FILE):
            os.remove(PROGRESS_FILE)
            print("ğŸ§¹ Checkpoint temporal eliminado (proceso completo).")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == '__main__':
    procesar_video()